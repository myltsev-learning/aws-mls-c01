A popular activation function that outputs the input directly if it's positive and zero otherwise. It's computationally efficient but can suffer from the "dying ReLU" problem when negative inputs become inactive.

![[Deep Learning 101 (5)]]

![[Leaky ReLU]]

#card 